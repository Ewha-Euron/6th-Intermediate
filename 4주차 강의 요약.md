# 4. 얕은 신경망 네트워크
## 4-6. 활성화 함수
- __sigmoid__
 
  -이진 분류 출력층 외에는 거의 쓰지 말기
  
![sigmoid 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/sigmoid%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)
![sigmoid](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/sigmoid.png?raw=true)

- __Tanh__

  -값이 [-1, 1] 사이에 있고 평균이 0이라 데이터를 원점으로 이동하는 효과 존재 → 다음층 학습 더 쉬워짐
  -은닉층에서 시그모이드 함수보다 더 좋음
  
![Tanh 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/Tanh%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)
![Tanh](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/Tanh.png?raw=true)

- __ReLU__

  -활성화 함수 기본값으로 많이 사용(이진분류 출력층 제외)
  -0보다 큰 활성화 함수의 미분값이 다른 함수에 비해 많아서 빠르게 학습 가능
  
![ReLU 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/ReLU%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)![ReLU](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/ReLU.png?raw=true)

- __leaky ReLU__

  -많이 쓰이진 않지만, ReLU보다 성능 좋음
  
![ReLU 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/leaky%20ReLU%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)
![ReLU](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/leaky%20ReLU.png?raw=true)

## 4-7. 왜 비선형 활성화 함수를 써야할까요?
두 선형 함수의 조합은 하나의 선형함수이므로, 은닉층을 쌓아도 아무런 혜택 x

∴은닉층에는 비선형 활성화 함수를 사용

## 4-8. 활성화 함수의 미분
- __sigmoid__

![sigmoid 미분 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/sigmoid%20%EB%AF%B8%EB%B6%84%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)
![sigmoid 미분](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/sigmoid%20%EB%AF%B8%EB%B6%84.png?raw=true)

- __Tanh__

![Tanh 미분 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/Tanh%20%EB%AF%B8%EB%B6%84%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)
![Tanh 미분](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/Tanh%20%EB%AF%B8%EB%B6%84.png?raw=true)

- __ReLU__

![ReLU 미분 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/ReLU%20%EB%AF%B8%EB%B6%84%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)
![ReLU 미분](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/ReLU%20%EB%AF%B8%EB%B6%84.png?raw=true)

- __leaky ReLU__
  
![ReLU 미분 그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/leaky%20ReLU%20%EB%AF%B8%EB%B6%84%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)
![ReLU 미분](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/leaky%20ReLU%20%EB%AF%B8%EB%B6%84.png?raw=true)

## 4-9. 신경망 네트워크와 경사 하강법

단일층 신경망에서 경사 하강법을 구현하기 위한 방법

![단일층 신경망 경사 하강법 구현](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/%EB%8B%A8%EC%9D%BC%EC%B8%B5%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B2%BD%EC%82%AC%20%ED%95%98%EA%B0%95%EB%B2%95%20%EA%B5%AC%ED%98%84.png?raw=true)

단일층이 아닐 때는 1, 2, …,m 까지의 계산을 반복

## 4-10. 역전파에 대한 이해
역전파 구현 TIp: 차원이 정확히 일치하는지 확인

로지스틱 회귀의 역전파

![로지스틱 회귀 역전파](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week4/images/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80%20%EC%97%AD%EC%A0%84%ED%8C%8C.png?raw=true)

## 4-11. 랜덤 초기화

신경망에서 w의 초기값을 0으로 설정한 후 경사 하강법을 적용하면 올바르게 작동 x  ∵dw 를 계산했을 때 모든 층이 같은 값을 가지게 돼서

∴np.random.rand()를 이용해 0이 아닌 랜덤한 값을 부여
