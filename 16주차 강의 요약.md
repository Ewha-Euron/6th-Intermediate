# 자연어 처리의 모든 것
## 1. 자연어 처리의 시작 
### 1-1. 자연어 처리 개요
#### 1-1-1. 자연어 처리 활용 분야와 트렌드
1. Natural language processing (자연어 처리)
- 주요 학회 : ACL, EMNLP, NAACL
- 학문 분야
  - Low-level parsing : Tokenization, stemming
  - Word and phrase level : NER(Named Entity Recognation), POS(Part-Of-Speech) tagging
  - Sentence level : 감성 분류(Sentiment Analysis), 기계 번역(Machine Translation)
  - Multi-sentence and paragraph level : 논리적 내포 및 모순관계 예측(Entailment Prediction), 독해기반 질의응답(question answering), 챗봇(dialog systems), 요약(summarization)


2. Text mining (텍스트 마이닝)
- 주요 학회 : KDD, The WebConf(前 WWW), WSDM, CIKM, ICWSM
- 학문 분야
  - Extract useful information and insights from text and document data
  - 문서 군집화(Document clustering)      ex) 토픽 모델링
  - Highly related to computational social science : 통계적으로 사회과학적 인사이트 산출

3. Information retrieval (정보 검색)
- 주요 학회 : SIGIR, WSDM, CIKM, Recsys
- 학문 분야
  - Highly related to computational social science
  - 정보 검색 분야, 추천 시스템
 
- 자연어 처리 분야의 트렌드
  - 인공지능과 딥러닝 기술이 가장 활발히 적용되며 꾸준비 발전하는 분야
  - 워드 임베딩(Word Embedding): 주어진 텍스트 데이터를 숫자로 변환, 자연어 처리 문제를 해결 위해 필요한 과정
  - 순서 정보가 중요 ex) RNN(Recurrent Neural Network), LSTM, GRU
  - 셀프 어텐션(Self-Attention) 구조를 가진 트랜스포머(Transformer) 모델 각광
  - 자가지도 학습(self-supervised Learning) 가능 모델 유행 ex) BERT, GPT

#### 1-1-2. 기존의 자연어 처리 기법
- Bag-Of-Words (단어 가방 모형)
  - 단어들의 출현 빈도(frequency)에만(단어들의 순서x) 집중하는 텍스트 데이터의 수치화 표현 방법
  - 단어 벡터로 표현하기 위해, 문장에 쓰인 단어들을 사전(Vocabulary) 형태로 저장, 중복 허용x
  - 저장된 단어들은 카테고리 변수(Categorical variable)이므로, 원-핫 인코딩(One-hot Encoding)를 이용해 벡터로 표현 → 문장을 숫자(원-핫 벡터의 합)로 표현 가능

- Naive Bayes Classifier for Document Classification(나이브 베이즈 분류기)
 - 머신러닝의 주요 알고리즘(인공 신경망 알고리즘x), 준수한 성


### 1-2. 자연어 처리와 벡터
#### 1-2-1. Word Embedding - (1)Word2Vec
- Word Embedding
  - 각 단어를 좌표공간에 최적의 벡터로 표현하는(임베딩하는) 기법
  - 최적의 좌표값; 유사한 단어는 가까이, 유사하지 않은 단어는 멀리 위치하는 것을 <U>최적의 좌표값</U>으로 표현가능
 
- Word2Vec Idea
  - 주변에 등장하는 단어들을 통해 중심 단어의 의미가 표현될 수 있다는 것으로 추정 시작
  - 워드를 토크나이징(Tokenizing; 문자(Text)를 컴퓨터가 이해할 수 있는 Token이라는 숫자 형태로 바꿔주는 행위) → 유니크한 단어만 모아서 사전(Vocabulary) 생성 → 문장에서 중심단어를 위주로 학습 데이터를 구축

- Word2Vec의 계산
  - 문장의 단어의 개수만큼 Input, Output 벡터 사이즈를 입,출력, 히든 레이어(hidden layer,은닉 층)의 차원(dim)은 사용자가 파라미터로 지정 가능
  - 마지막 결과값으로 나온 벡터는 softmax 연산;  가장 큰 값=1, 나머지=0

- Application of Word2Vec
  - Machine translation : 단어 유사도를 학습하여 번역 성능 더 ↑
  - Sentiment analysis : 감정분석, 긍부정분류 도움
  - Image Captioning : 이미지의 특성을 추출해 문장으로 표현하는 테스크 도움

#### 1-2-2. Word Embedding - (2)GloVe
-  Glove(Global Vectors for Word Representation)
    - (Word2Vec과 다르게) 사전에 미리 각 단어들의 동시 등장 빈도수를 계산하며, 단어간의 내적값과 사전에 계산된 값의 차이를 줄여가는 형태로 학습
    - (Word2Vec는 모든 연산을 반복) Glove는 사전에 계산된 Ground Truth를 사용해 반복계산 ↓

  →Word2Vec보다 더 빠르게 동작, 더 적은 데이터에서도 잘 동작

- 사전 학습된 Glove 모델
  - 사전에 이미 대규모 데이터로 학습된 모델이 오픈소스로 공개
  - uncased: 대,소문자 구분x
  - [Glove 깃헙 주소](https://github.com/stanfordnlp/GloVe)

### 1-2. 자연어 처리와 딥러닝
#### 1-2-1. Recurrent Neural Network (RNN)
- 현재 타임스텝에 대해 이전 스텝까지의 정보를 기반으로 예측값을 산출하는 구조의 딥러닝 모델
- 매 타임스텝마다 동일한 파라미터를 가진 모듈을 사용하므로, '재귀적인 호출'의 특성 보여줌
![RNN](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week16/images/1-2-1.%20RNN.png?raw=true)

![RNN 계산 방법](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week16/images/1-2-1.%20RNN%20%EA%B3%84%EC%82%B0%20%EB%B0%A9%EB%B2%95.png?raw=true)

![다양한 타입의 RNN 모델](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week16/images/1-2-1.%20%EB%8B%A4%EC%96%91%ED%95%9C%20%ED%83%80%EC%9E%85%EC%9D%98%20RNN%20%EB%AA%A8%EB%8D%B8.png?raw=true)

#### 1-2-2. Character-level Language Model
- 언어 모델: 이전에 등장한 문자열을 기반으로 다음 단어를 예측하는 태스크
    - 캐릭터 레벨 언어 모델(character-level Language Model): 문자 단위로 다음에 올 문자를 예측하는 언어 모델
    - logit: 각 타임스텝별로 output layer를 통해 유니크한 문자 개수만큼의 차원인 벡터를 출력
    - softmax layer를 통과시키면 원-핫 벡터 형태의 출력값 산출
 
![예시](https://cphinf.pstatic.net/mooc/20220930_99/16644919811126z3pI_PNG/mceclip1.png)

#### 1-2-3. Backpropagation through time and Long-Term-Dependency
- RNN 모델이 학습하는 방법 : Truncation , BPTT
- Truncation: 제한된 리소스(메모리) 내에서 모든 시퀀스를 학습할 수 없어, 잘라서 학습에 사용하는 것
  <br>
  ![Truncation](https://cphinf.pstatic.net/mooc/20220930_262/1664494307725RzlQT_PNG/mceclip0.png)
- 딥러닝 모델은 forward propagation을 통해 계산된 W를, backward propagation을 지나면서 W를 미분한 값인 gradient를 통해 학습
- BPTT(Backpropagation through time): RNN에서 타임스텝마다 계산된 weight를 backward propagation을 통해 학습하는 방식을 의미
- Long-Term-Dependency 문제: gradient가 전파되면서 소실되거나 증폭되면서 멀리까지 학습정보를 잘 전달x → LSTM 모델: RNN의 Long-Term-Dependency 문제 보완

#### 1-2-4. Long Short-Term Memory (LSTM)
- 단기 기억으로 저장하여 이걸 때에 따라 꺼내 사용함으로 더 오래 기억할 수 있도록 개선
- Cell state에는 핵심 정보들을 모두 담아두고, 필요할 때마다 Hidden state를 가공해 time step에 필요한 정보만 노출하는 형태로 정보가 전파
  <br>
  ![LSTM](https://cphinf.pstatic.net/mooc/20220930_19/16644982796745KHg8_PNG/mceclip0.png)

![LSTM의 여러 gate 설명](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week16/images/1-2-4.%20LSTM%EC%9D%98%20%EC%97%AC%EB%9F%AC%20gate%20%EC%84%A4%EB%AA%85.png?raw=true)

- RNN과 다른점
  - 각 time step마다 필요한 정보를 단기 기억으로 hidden state에 저장하여 관리되도록 학습
  - 오차역전파(backpropagation) 진행시 가중치(W)를 계속해서 곱해주는 연산이 아니라, forget gate를 거친 값에 대해 필요로하는 정보를 덧셈을 통해 연산하여 그레디언트 소실/증폭 문제를 방지

![GRU](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week16/images/1-2-4.%20GRU.png?raw=true)

- RNN , LSTM , GRU 요약
  - RNN: 들어오는 입력값에 대해서, 많은 유연성을 가지고 학습되는 딥러닝 모델
  - RNN에서는 그레디언트 소실/증폭 문제가 있어 실제로 많이 사용되지는 않지만, RNN 계열의 LSTM, GRU 모델은 현재도 많이 사용
  - LSTM과 GRU 모델은 RNN과 달리 가중치를 곱셈이 아닌 덧셈을 통한 그레디언트 복사로 그레디언트 소실/증폭 문제를 해결
