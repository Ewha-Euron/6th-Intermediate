# 1\. 파이썬 브로드캐스팅

파이썬 코드와 실행 시간을 단축할 수 있는 테크닉

3x4 크기의 행렬 A가 있다. 열은 음식 종류, 행은 성분으로 이루어져 있다. 각 음식에서 각 성분의 백분율을 구하자.

- 각 열의 합을 구해야 한다.
- 각 원소를 열의 합으로 나누고 100을 곱해야 한다.

\-> 브로드캐스팅을 통해 for문 없이 연산할 수 있다.

```
print(A.shape)  # (3, 4)

print(A)
# [[ 56.    0.    4.4  68. ]
#  [  1.2 104.   52.    8. ]
#  [  1.8 135.   99.    0.9]]
```

💡 \`axis=0\` : 첫 번째 차원 =행 \`axis=1\` : 두 번째 차원 =열

```
# 첫 번째 차원(행)끼리 더하기 -> 각 열의 합 계산을 한 번에
cal = A.sum(axis=0)
print(cal)

# [ 59.  239.  155.4  76.9]
```

```
# 각 원소에 100을 곱하고 합으로 나누는 계산을 한 번에
percentage = 100 * A / cal
# reshape로 차원을 확실히 맞춰주기
# percentage = 100 * A / (cal.reshape(1, 4))
print(percentage)

# [[94.91525424  0.          2.83140283 88.42652796]
# [ 2.03389831 43.51464435 33.46203346 10.40312094]
# [ 3.05084746 56.48535565 63.70656371  1.17035111]]
```

차원이 다른 array끼리 브로드캐스팅을 통해 연산하기

- \-, +, -, \*, / 연산: 차원 맞춰주고 element-wise 연산

```
a = np.array([1,2,3,4]).reshape(4,1)
print(a)

# [[1]
#  [2]
#  [3]
#  [4]]

# 상수를 같은 차원으로 확장하여 합
print(a+100)

# [[101]
#  [102]
#  [103]
#  [104]]
```

```
# mxn 행렬
a = np.array([[1,2,3], [4,5,6]]).reshape(2,3)
# 1xn 행렬을 복제해서 mxn 행렬로 만들고 합
b = np.array([100, 200, 300]).reshape(1,3)
print(a)

# [[1 2 3]
#  [4 5 6]]

print(b)

# [[100 200 300]]

print(a+b)

# [[101 202 303]
#  [104 205 306]]
```

```
# mx1 행렬을 복제해서 mxn 행렬로 만들고 합
b = np.array([100,200]).reshape(2,1)
print(a+b)

# [[101 102 103]
#  [204 205 206]]
```

# 2\. 파이썬과 넘파이 벡터

파이썬 넘파이 코드를 간단하게 하거나 오류를 잘 해결하기 위한 코드

넘파이의 벡터 연산은 매우 편리하지만 의도치 않은 계산 결과나 오류를 만날 수 있다.

넘파이에서 단순히 원소가 5개인 배열을 랭크가 1인 배열(**rank 1 array**)라고 한다.

- 행 벡터도 열 벡터도 아니다.
  - transpose나 행렬 연산이 안된다.
- 결과가 직관적이지 않다.

→ 신경망을 코딩할 때는 이와 같은 랭크 1 배열을 아예 사용하지 않기를 권장!

→ 대신 nx1 or 1xn 의 백터를 사용할 수 있다.

```
# rank 1 array
a = np.array([1, 2, 3, 4, 5])
print(a, a.shape)

# [1 2 3 4 5] (5,)
```

```
a = a.reshape(5,1)
print(a, a.shape)

# [[1]
#  [2]
#  [3]
#  [4]
#  [5]] (5, 1)
```

벡터는

- `[[ ]]` 형태이다.
- transpose와 행렬 연산이 가능하다.

matrix, vector를 원하는 차원으로 만들기 위해 `reshape()` 함수를 사용

벡터와 차원이 확실하지 않을 때는 확실히 하기 위해 `assert()` 함수를 사용

- 연산 비용이 저렴하니 오류를 막기 위해 사용하는 것을 권장!

```
assert(a.shape == (5,1))
# 맞지 않으면 AssertionError
```

# 3\. 로지스틱 회귀의 Cost function

로지스틱 회귀의 cost function 자세히 살펴보기

$\\hat{y}$ = y가 1일 확률

$1-\\hat{y}$ = y가 0일 확률

if y=1: $P(y|x)=\\hat{y}$

if y=0: $P(y|x)=1-\\hat{y}$

$P(y|x) = \\hat{y}^y(1-\\hat{y})^{(1-y)}$

y=1이면 첫 항은 $\\hat{y}$, 둘째 항은 1

y=0이면 첫 항은 1, 둘째 항은 1-$\\hat{y}$

log P(y|x)를 최대화하는 것은 P(y|x)를 최대화하는 것과 유사하다.

$log\\ \\hat{y}^y(1-\\hat{y})^{(1-y)} = ylog\\hat{y}+(1-y)log(1-\\hat{y}) = -\\mathcal{L}(\\hat{y}, y)$

음수로 표현되는 이유는 일반적인 train에서는 확률을 높이려고 하지만 로지스틱 회귀에서는 손실 함수를 최소화하려고 하기 때문이다.

- 손실함수 최소화 == 확률 최대화

전체 샘플 m에 대한 cost function

- 모든 train sample들이 독립동일분포(independent and identically distributed, iid)라고 가정하면 전체 샘플에 대한 확률은 각 확률의 곱

(_labels_ _in_ _training_ _set_)=∏*i*\=1*m\*\*P*(_y_(_i_)∣*x*(_i_))  
$logP(_labels_ _in_ _training_ _set_)=log∏_i_\=1_m\*\*P_(_y_(_i_)∣_x_(_i_)) =−∑_i_\=1_m\*\*L_(_y_^(_i_),_y_(_i_))$  
cost function: $_J_(_w_,_b_)=_m_1∑_i_\=1_m\*\*L_(_y_^(_i_),_y_(_i_))$

maximum likelihood estimation(최대 우도 추정의 원칙)

- \=위 식의 값을 최대화하는 파라미터 선택하기

스케일을 맞추기 위해 1/m

- 누적해서 합하면 스케일이 커지기 때문에 m으로 나누어 스케일링
- 평균을 구하는 것과 유사

cost function 최소화 → $−∑_i_\=1_m\*\*L_(_y_^(_i_),_y_(_i_))$ 최대화 → 확률 최대화

→ 요약하자면 cost function J(w, b)를 최소화함으로써 로지스틱 회귀 모델의 최대 우도 추정한 것
