# 딥러닝 4단계: 합성곱 신경망 네트워크 (CNN)
## 1. 합성곱 신경망
### 1-1. 컴퓨터비전
- 다양한 분야에 응용; 얼굴인식, 예술, 새로운 비전 관련 어플리케이션을 창출, 자연어 처리
- 주로 다루는 분야: 이미지분류, 객체 인식, 신경망 스타일 변형 등
- 큰 단점: 입력데이터가 아주 큼 but 합성곱 연산에는 해당 x
  
### 1-2. 모서리 감지 예시
- 프로그래밍 언어 구현시 별도의 함수 존재
- 이미지: (높이 x 넓이)
- 왼쪽: 원래 이미지, 중앙: 필터(커널) → 합성곱 연산
  <br>
  ![합성곱](https://cphinf.pstatic.net/mooc/20190115_192/15475273072333zzE5_PNG/conv1.png)
- 필터(커널)을 한칸 이동하여 합성곱 연산
  <br>
![합성곱](https://cphinf.pstatic.net/mooc/20190115_20/1547527311549DG2cx_PNG/conv2.png)

- 필터를 통과해 합성곱 연산 후 나타나는 밝은 중앙 부분이 원래 이미지의 경계선을 해당 하는 부분
  <br>
  이미지 크기 더 커지면, 정교하게 찾는 거 ㄱㄴ

  ![수직 윤곽선 필터](https://cphinf.pstatic.net/mooc/20181024_65/1540364817391EIR58_PNG/edgedetact.png)
  
### 1-3. 더 많은 모서리 감지 예시
- 다양한 필터 존재
- 최근 딥러닝에서는 문제에 적합한 필터를 만드는 방법을 사용 by 임의의 숫자로 만든 다음에 역전파를 통해 알아서 학습시켜서
### 1-4. 패딩 (Padding)
- 이미지 축소, 이미지 윤곽 정보 거의 소실 문제 해결 가능
- 이미지 주위에 추가로 하나의 경계를 덧대는 것, 보통 0 사용
- 최종 이미지 크기: (n + 2p - f + 1) x (n + 2p - f + 1); n: 이미지 크기, p: 패딩 크기, f: 필터 크기
- __유효 합성곱(valid convolution)__: 패딩 x,
- __동일 합성곱(same convolution)__: 기존 이미지 크기=패딩 후 이미지 크기
- 필터의 크기: 홀수 → 동일한 크기로 패딩 추가 가능, 중심 위치 존재. 3x3이 일반적 
### 1-5. 스트라이드 (Stride)
- 스트라이드: 필터의 이동 횟수
- 스트라이드 최종 크기, 소숫점은 내림
![스트라이드 최종 크기](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week15/images/1-5%20%EC%8A%A4%ED%8A%B8%EB%9D%BC%EC%9D%B4%EB%93%9C%20%EC%B5%9C%EC%A2%85%20%ED%81%AC%EA%B8%B0.png?raw=true)
- 딥러닝에서는 교차상관을 합성곱이라고 함, 신호처리에서만 유용한 뒤집는 연산 생략
### 1-6. 입체형 이미지에서의 합성곱
- 이미지: (높이 x 넓이 x 채널), 채널: 색상 or 입체형 이미지 깊이
- 필터도 각 채널 별로 하나씩 ↑, 각 채널 별로 필터 동일하게, 다르게 사용 가능
![입체형 합성곱의 연산](https://cphinf.pstatic.net/mooc/20181024_84/1540365723643PnRKO_PNG/conv_3d.PNG)

![최종 결과](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week15/images/1-6%20%EC%B5%9C%EC%A2%85%20%EC%B6%9C%EB%A0%A5.png?raw=true)
ㅡㅡㅡㅡㅡㅡㅡㅡㅡ
### 1-7. 합성곱 네트워크의 한 계층 구성하기
- 합성곱 신경망의 한 계층 구성: 합성곱 연산 → 편향 추가 → 활성화 함수(비선형성 적용 위해, ReLU 많이 이용)
![1-7](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week15/images/1-7.png?raw=true)
### 1-8. 간단한 합성곱 네트워크 예시
간단한 합성곱 신경망 구조 예시
![간단한 합성곱 신경망 구조 예시](https://cphinf.pstatic.net/mooc/20181024_189/1540366347640iSxJm_PNG/conv_process.PNG?type=w760)
- 신경망 깊이 ∝ 채널의 수/합성곱 신경망 크기
- 합성곱 디자인 하는 일: 하이퍼파라미터(필터의 크기, 스트라이드, 패딩, 필터의 개수 등) 선택하는 과정
- 합성곱 층, 풀링 층, 완전 연결 층으로 구성

### 1-9. 풀링(Pooling)층
- 계산 속도↑, 특징 더 잘 검출 가능
- 보통 최대 풀링 이용
- p=0, 앞의 공식 사용 가능
- 하이퍼파라미터: f, s, max or average pooling, (p)
 
### 1-10. CNN 예시
- 합성곱 층과 풀링 층을 각각의 층으로 간주하는 관습
- 합성곱 층과 풀링 층을 하나의 층으로 간주하는 관습 ∵풀링 층에 학습해야 할 변수 존재x

  ![LeNet-5](https://cphinf.pstatic.net/mooc/20181024_208/1540366636959M9fMo_PNG/lenet-5.PNG)

### 1-11. 왜 합성곱을 사용할까요?
- 합성곱 신경망을 사용하면 변수를 적게 사용 가능
  <br>
  ∵ 변수 공유; 필터가 다른 이미지에서도 똑같이 적용 or 도움
   <br>
  ∵ 희소 연결; 출력값이 이미지의 일부(작은 입력값)에 영향을 받고, 나머지 픽셀들의 영향을 받지 x → 과대적합 방지
- 이동 불변성을 포착에 용이; 이미지 약간의 변형도 포착 가능 ex) 고양이 사진이 몇 픽셀 이동해도 여전히 고양이
