# 딥러닝 2단계: 심층 신경망 성능 향상시키기
## 5. 하이퍼파라미터 튜닝
### 5-1. 튜닝 프로세스(tuning process)
- 딥러닝에 존재하는 하이퍼파라미터(우선 조정하는 순서)
  - 학습률(α)
  - 모멘텀(Momentum) 알고리즘의 β
  - 은닉 유닛의 수
  - 미니배치 크기
  - 은닉층의 갯수
  - 학습률 감쇠(learning rate decay) 정도
  - 아담(Adam) 알고리즘의 $β_1$, $β_2$, ϵ

- 하이퍼파라미터의 탐색: 격자점x, 무작위 접근 방식으로 ∵ 어떤 하이퍼파라미터가 문제 해결에 더 중요한지 미리 알 수 x
  
- 정밀화 접근: 우선 전체 하이퍼파라미터 공간에서 탐색하여 좋은 점을 찾은 후, 그 근방에서 더 정밀하게 탐색하는 과정

### 5-2. 적절한 척도 선택하기(using an appropriate scale to pick hyperparameters)

무작위 ← 적절한 척도 정해서
- 무작위로 뽄는 것이 합리적인 하이퍼파라미터들 존재 ex) 은닉 유닛의 수($n^{[l]}$, 은닉층의 수(#layers)
- 학습률: 선형척도대신 로그척도에서 하이퍼파라미터를 찾는 것이 합리적
  
    r ∈ [a,b], $a=\log_{10}{(작은 값)}$, $b=\log_{10}{(큰 값)}$
  <br>
    $α=10^r$
- 지수 가중 이동 평균에서 사용되는 β: 1-β 취한 후, 로그척도에서 무작위 값을 선택하여 탐색
  
    r ∈ [a,b], $a=\log_{10}{(작은 값)}$, $b=\log_{10}{(큰 값)}$
  <br>
    $1-β=10^r$ → $β=1-10^r$
- 선형척도에서 샘플을 뽑은 것이 좋지 않은 이유: 1에 가까울 수록 알고리즘 결과에 더 큰 영향을 끼쳐서

### 5-3. 하이퍼파라미터 튜닝 실전(hyperparameters tuning in practice: Pandas vs Caviar)

하이퍼파라미터 찾을 때 사용하는 방법
1. 모델 돌보기(baby sitting one model) = 판다 접근
   - 하나의 모델로 매일 성능을 지켜보면서, 학습 속도를 조금 씩 바꾸는 방식
   - 컴퓨터의 자원이 많이 필요하지 않거나, 적은 숫자의 모델을 한번에 학습 시킬 수 있을 때 사용
2. 동시에 여러 모델 훈련(Training many models in parallel) = 캐비어 접근
   - 서로 다른 모델을 동시에 학습 후 마지막에 최고 성능을 보이는 것을 선택
   - 컴퓨터의 자원이 충분히 많아 여러 모델을 한번에 학습 시킬 수 있을 때 사용

## 6. 배치 정규화(Batch Normalization)
### 6-1. 배치 정규화(normalizing activations in a network)
- 하이퍼파라미터에 상관없이 튼튼한 신경망을 만드는 방법
- 모든 신경망에 적요 x, 적용가능하다면 하이퍼파라미터 탐색 훨씬 쉽고 빨라짐
- 신경망과 하이퍼파라미터의 상관관계 ↓
- 입력층뿐만 아니라 은닉층 값들까지도 정규화 ← 은닉 유닛의 평균과 표준편차를 γ, β 이용해 특정한 평균, 분산 갖도록 정규화
- 보통 $a^{[l]}$(활성 함수 이후의 값)보단 $z^{[l]}$(활성 함수 이전의 값) 정규화

![작동원리](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week11/images/6-1.%20%EB%B0%B0%EC%B9%98%20%EC%A0%95%EA%B7%9C%ED%99%94%20%EC%9E%91%EB%8F%99%EC%9B%90%EB%A6%AC.png?raw=true)

→ γ, β: 학습과정에서 학습하는 파라미터

정규화 이후 다시 선형변환하는 이유: 항상 같은 분포 값을 갖지 않게 하기 위함

### 6-2. 배치 정규화 적용시키기(fitting batch norm into a neural network)
- 은닉층에서 2단계
  1. 선형결합인 z 계산, 이를 배치 정규화
  2. 정규화 된 값들을 활성화 함수를 거쳐 활성화 값 a 얻음
- 선형결합 단계에서 상수항 b 는 없어짐 ∵배치 정규화 과정에서 z의 평균을 빼주면 사라짐

![경사 하강법 실행](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week11/images/6-2%20%EA%B2%BD%EC%82%AC%20%ED%95%98%EA%B0%95%EB%B2%95%20%EC%8B%A4%ED%96%89.jpg?raw=true)

### 6-3. 배치 정규화가 잘 작동하는 이유는 무엇일까요?(why does batch norm work?)
배치 정규화는 입력특성 X의 평균을 0, 분산을 1로 만듦으로써 학습 속도를 빠르게 함
- 이전 층의 가중치 영향을 덜 받게 ∵은닉층 값의 분포 변화를 줄여줘서, 입력 값의 분포를 제한하기 때문
  <br>
  *공변량 변화(covariant shift): 데이터 분포과 변화하는 것
  <br>
  → 배치 정규화는 입력값이 바뀌어서 발생하는 문제를 안정화
   <br>
  → 앞층과 뒷층의 매개변수의 상관 관계를 줄여주기 때문에, 학습속도 향상 가능
- 파라미터의 정규화(regularization)
  - 미니배치로 계산한 평균과 분산은 전체 데이터의 일부으로 추정한 것이기 때문에 잡음 존재
  - 배치 정규화: 곱셈 잡음($\times  \frac{1}{σ}$), 덧셈 잡음(+(−μ)) 존재
  - 은닉층에 잡음 추가 → 이후 은닉층이 하나의 은닉 유닛에 너무 의존하지 않도록 만듦
  - 미니배치 ↑ → 정규화 효과
    <br> 
   *배치 정규화를 일반화를 목적으로 사용 x

### 6-4. 테스트시의 배치 정규화(batch norm at test time)

배치 정규화는 한 번에 미니배치 하나의 데이터를 다룸
- 테스트 시에는 한 번에 샘플 하나씩 처리 → 배치가 하나여서 평균. 분산 계산 x
  <br>
  → ∴학습시에 사용된 미니배치들의 지수 가중 이동 평균을 추정치로 사용(=이동 평균)
