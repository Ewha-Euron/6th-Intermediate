{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNodAlQT5EkNo7lg6CmFDQ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mons-trev/6th-Intermediate/blob/Week11/%EB%B0%B0%EC%B9%98%EC%A0%95%EA%B7%9C%ED%99%94%EC%BA%90%EA%B8%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JemOCmYjU7CQ",
        "outputId": "41667738-358c-4dab-e4df-891241dfffb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading titanic, 34877 bytes compressed\n",
            "\r[==================================================] 34877 bytes downloaded\n",
            "Downloaded and uncompressed: titanic\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'titanic:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F3136%2F26502%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240518%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240518T142955Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D983f9262dc92925b9bd0ecbfa327888e949d6deb6cfad4aafbddb927fb04fe461781200dc1040f0073113d800ff3ef57dabb6bbaccc4839e167de101f58b50b72746b30e25b35d0ce04e4c9b7fbaf38b4b6d50721dacc2e897396b60b7680998986dff654970613f721bf2a9143b1ae286209b300cf210fccdec490b0ef295dc47409bcdc3dc5d616ad9dbec592b2c80933ec63d89655671f0fdabfe9d9bba9f510902faa0dd58a87604bfff26f50030578a001a6d9dc6c6923231348638f9e162f3d0f9b5aa3a230ad5f1ae2b9b44360b0324fbfe57fb38c6c538a94d2afd558178700e13f656bbe3ee9d0afb5e84638d1781c6678638583bec86b6bd136649'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
        "submission = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')"
      ],
      "metadata": {
        "id": "b7XydxzNVC8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(5, 128),\n",
        "            ## Batch Normalization between 'Layer' and 'Activation function'\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            ## Drop out after 'Activation function'\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0G3evO_3VE-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = pd.concat((train.drop(['Survived'], axis = 1), test), axis = 0)\n",
        "\n",
        "data_set = data_set.drop(['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis = 1)\n",
        "data_set = data_set.fillna(data_set.mean())\n",
        "\n",
        "n_train = train.shape[0]\n",
        "train_x, test_x = data_set[:n_train], data_set[n_train:]\n",
        "train_y = train['Survived']\n",
        "\n",
        "train_x = train_x[train_x.keys()].values\n",
        "test_x = test_x[test_x.keys()].values\n",
        "train_y = train_y.values\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "simple_nn = SimpleNN()\n",
        "optimizer = optim.Adam(simple_nn.parameters(), lr=0.01)\n",
        "error = nn.BCELoss()\n",
        "\n",
        "batch_size = 99\n",
        "batch_count = int(len(train_x) / batch_size)\n",
        "\n",
        "for epoch in range(300):\n",
        "    train_loss = 0\n",
        "    num_right = 0\n",
        "    for i in range(batch_count):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        tensor_x = torch.FloatTensor(train_x[start:end])\n",
        "        tensor_y = torch.FloatTensor(train_y[start:end]).reshape(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = simple_nn(tensor_x)\n",
        "        loss = error(output, tensor_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * batch_size\n",
        "        result = [1 if out >= 0.5 else 0 for out in output]\n",
        "        num_right += np.sum(np.array(result) == train_y[start:end])\n",
        "\n",
        "    train_loss = train_loss / len(train_x)\n",
        "    accuracy = num_right / len(train_x)\n",
        "\n",
        "    if epoch % 25 == 0:\n",
        "        print('Loss: {} Accuracy: {}% Epoch:{}'.format(train_loss, accuracy, epoch))\n",
        "\n",
        "print('Training Ended')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qgbNnSxVIKO",
        "outputId": "4f7c0303-84a2-4192-99e0-24037c65f0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6447252697414823 Accuracy: 0.6464646464646465% Epoch:0\n",
            "Loss: 0.5216609802511003 Accuracy: 0.755331088664422% Epoch:25\n",
            "Loss: 0.45395266347461277 Accuracy: 0.7912457912457912% Epoch:50\n",
            "Loss: 0.4057227472464244 Accuracy: 0.819304152637486% Epoch:75\n",
            "Loss: 0.39391301075617474 Accuracy: 0.8204264870931538% Epoch:100\n",
            "Loss: 0.33585229847166276 Accuracy: 0.8529741863075196% Epoch:125\n",
            "Loss: 0.31881627937157947 Accuracy: 0.8608305274971941% Epoch:150\n",
            "Loss: 0.31529144446055096 Accuracy: 0.8630751964085297% Epoch:175\n",
            "Loss: 0.28890813224845463 Accuracy: 0.8709315375982043% Epoch:200\n",
            "Loss: 0.25704990824063617 Accuracy: 0.8900112233445566% Epoch:225\n",
            "Loss: 0.2314977612760332 Accuracy: 0.9057239057239057% Epoch:250\n",
            "Loss: 0.21851203673415714 Accuracy: 0.9057239057239057% Epoch:275\n",
            "Training Ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_test_x = torch.FloatTensor(test_x)\n",
        "with torch.no_grad():\n",
        "    test_output = simple_nn(tensor_test_x)\n",
        "    result = np.array([1 if out >= 0.5 else 0 for out in test_output])\n",
        "    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': result})\n",
        "    submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "WgihOKn3VI6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}