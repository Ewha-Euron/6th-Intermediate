# 딥러닝 2단계: 심층 신경망 성능 향상시키기
## 4. 최적화 알고리즘
### 4-1. 미니 배치 경사하강법(mini-batch gradient descent)
- 배치 경사 하강법: 전체 훈련 샘플에 대해 훈련 후 경사 하강 진행 → 큰 데이터 세트 훈련하는데 많은 시간이 들어 경사 하강을 진행하기까지 오랜 시간이 소요
- 미니배치 경사 하강법: 전체 훈련 샘플을 작은 훈련 세트인 미니배치로 나눈 후, 미니배치 훈련 후 경사 하강 진행
  
![표기법](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/%EB%AF%B8%EB%8B%88%20%EB%B0%B0%EC%B9%98%20%ED%91%9C%EA%B8%B0%EB%B2%95.png?raw=true)

### 4-2. 미니 배치 경사하강법 이해하기(understanding mini-batch gradient descent)
![배치 경사 하강법](https://cphinf.pstatic.net/mooc/20180627_125/15300885625555rVb5_PNG/1.PNG?type=w760)
![미니배치 경사 하강법](https://cphinf.pstatic.net/mooc/20180627_148/1530088578609dbWSW_PNG/2.PNG?type=w760)
- 배치 경사 하강법: 매 반복마다 비용함수 J 값은 계속 감소
- 미니배치 경사 하강법: 전체적으로 비용 함수가 감소하는 경향, 많은 노이즈가 발생

- 미니배치 크기(mini-batch size): 가장 효율적이면서 비용함수 J를 줄이는 값을 찾아야 하는 하이퍼파라미터
  
  최적의 미니배치 크기: 1과 m(훈련 세트의 크기)의 중간값

### 4-3. 지수 가중 이동 평균(exponentially weighted average)
- 경사 하강법 및 미니배치 경사 하강법보다 더 효율적인 알고리즘을 이해하기 위해, 먼저 지수 가중 이동 평균 이해 필요
- 가중 이동 평균: 최근의 데이터에 더 많은 영향을 받는 데이터들의 평균 흐름을 계산, 최근 데이터 지점에 더 높은 가중치 부여
- β: 최적이 값을 찾아야 하는 하이퍼파라미터, 보통 0.9 사용
  
![식](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-3.%20%EC%A7%80%EC%88%98%20%EA%B0%80%EC%A4%91%20%EC%9D%B4%EB%8F%99%20%ED%8F%89%EA%B7%A0%20%EC%8B%9D.png?raw=true)

![vt의미](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-3.%20vt%EC%9D%98%EB%AF%B8.png?raw=true)

### 4-4. 지수 가중 이동 평균 이해하기(understanding exponentially weighted average)
초록색: β=0.98/  빨간색: β=0.9/  노란색: β=0.5

![그래프](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-4.%20%EA%B7%B8%EB%9E%98%ED%94%84.png?raw=true)

![학습내용](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-4.%20%ED%95%99%EC%8A%B5%20%EB%82%B4%EC%9A%A9.png?raw=true)

- 지수 가중 이동 평균의 장점: 구현 시 아주 적은 메모리를 사용

### 4-5. 지수 가중 이동 평균의 편향보정(bias correction in exponentially weighted average)
- 편향 보정(bias correction): 초기에 더 나은 추정값을 얻는데 도움, 평균을 더 정확하게 계산 가능
![학습내용](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-5.%20%ED%95%99%EC%8A%B5%EB%82%B4%EC%9A%A91.png?raw=true)

### 4-6. Momentum 최적화 알고리즘(gradient descent with momentum)
- 모멘텀 알고리즘 or 모멘텀이 있는 경사 하강법: 일반적인 경사 하강법보다 항상 더 빠르게 동작
  
  → 경사에 대한 지수가중평균을 계산한 값으로 가중치 업데이트

- 진동 막기 위해 수직 방향으로는 slower learning 필요, 최솟값 향해 가기 위해 수평 방향으로는 faster learning 필요
  
![알고리즘](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-6.%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98.png?raw=true)

- momentum의 장점: 매 단계의 경사 하강 정도를 부드럽게 만들어 줌
- momentum 알고리즘에서 보통 평향 추정 실행 X ∵ step이 10 단계정도 넘어가면, 이동평균은 준비가 돼서 편향 추정이 더 이상 일어나지 않음

### 4-7. RMSProp 최적화 알고리즘(RMSprop optimization algorithms)
- 미분값이 큰 곳에서 업데이트 시, 큰 값으로 나눠주기 때문에 기존 학습률보다 작은 값으로 업데이트 → 진동 감소(모멘텀과 비슷)
- 미분값이 작은 곳에서는 업데이트 시, 작은 값으로 나눠주기 때문에 기존 학습률보다 큰 값으로 업데이트 → 더 빠르게 수렴
  
![알고리즘](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-7.%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98.png?raw=true)

### 4-8. Adam 최적화 알고리즘(Adam optimization algorithms)
- Adam(Adaptive moment estimation) = Momentum + RMSProp
- 매우 넓은 범위의 아키텍처를 가진 서로 다른 신경망에서 잘 작동

![알고리즘](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-8.%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98.png?raw=true)

- 하이퍼파라미터
![하이퍼파라미터](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-8.%20%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.jpg?raw=true)

### 4-9. 학습률 감쇠(learning rate decay)
- 작은 미니배치일수록 잡음이 심해, 일정한 학습률이라면 최적값에 수렴하기 어려움
- 학습의 초기 단계에서는 큰 스텝으로 진행 → 학습이 수렴할수록, 학습률이 느려져 작은 스텝으로 진행
- 학습률 감쇠 기법 사용 이유: 점점 학습률을 작게 줘서 최적값을 더 빨리 찾도록 만드는 것

![다양한 학습률 감쇠 기법](https://github.com/seoyeonkim3/Euron-Intermediate-study/blob/Week10/images/4-9.%20%EB%8B%A4%EC%96%91%ED%95%9C%20%ED%95%99%EC%8A%B5%EB%A5%A0%20%EA%B0%90%EC%87%A0%20%EA%B8%B0%EB%B2%95.png?raw=true)

